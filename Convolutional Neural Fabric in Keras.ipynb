{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This implements Convolutional Neural Fabric (Shreyas Saxena, Jakob Verbeek, NIPS 16), a 3D trellis-shaped graph \"that embeds an exponentially large number of [convolutional network] architectures\" and abstracts away the problem of handcrafting \"a single optimal architecture.\"\n",
    "\n",
    "The original implementation uses Matlab to generate a Caffe network (prototext file). To the best of our knowledge, this is the first and only Keras/TensorFlow implementation so far.\n",
    "\n",
    "- Original paper: https://arxiv.org/abs/1606.02492.\n",
    "\n",
    "- Original implementation: https://github.com/shreyassaxena/convolutional-neural-fabrics\n",
    "\n",
    "-- Rishabh Kabra (rishabhkabra@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_109 (Sequential)      (None, 32, 32, 16)    512                                          \n",
      "____________________________________________________________________________________________________\n",
      "sequential_61 (Sequential)       (None, 16, 16, 16)    2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_62 (Sequential)       (None, 8, 8, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_63 (Sequential)       (None, 4, 4, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_64 (Sequential)       (None, 2, 2, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_73 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_72 (Sequential)       (None, 4, 4, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_67 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 4, 4, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_70 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_69 (Sequential)       (None, 8, 8, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 16, 16, 16)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_65 (Sequential)       (None, 1, 1, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_71 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 8, 8, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_66 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_76 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_75 (Sequential)       (None, 2, 2, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_68 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_83 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_82 (Sequential)       (None, 8, 8, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 2, 2, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 8, 8, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_80 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_74 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_81 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 16, 16, 16)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_78 (Sequential)       (None, 1, 1, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_86 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_85 (Sequential)       (None, 4, 4, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_79 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_93 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 1, 1, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 4, 4, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 16, 16, 16)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_77 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_84 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_92 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_89 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_88 (Sequential)       (None, 2, 2, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_96 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_95 (Sequential)       (None, 8, 8, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_97 (Sequential)       (None, 8, 8, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 2, 2, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_17 (Add)                     (None, 8, 8, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_87 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_94 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_91 (Sequential)       (None, 1, 1, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_100 (Sequential)      multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_99 (Sequential)       (None, 4, 4, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_101 (Sequential)      (None, 4, 4, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 1, 1, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_18 (Add)                     (None, 4, 4, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_90 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_98 (Sequential)       multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_104 (Sequential)      multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_103 (Sequential)      (None, 2, 2, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_105 (Sequential)      (None, 2, 2, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_19 (Add)                     (None, 2, 2, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_102 (Sequential)      multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_107 (Sequential)      (None, 1, 1, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "sequential_108 (Sequential)      (None, 1, 1, 16)      2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_20 (Add)                     (None, 1, 1, 16)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "sequential_106 (Sequential)      multiple              2384                                         \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            170                                          \n",
      "____________________________________________________________________________________________________\n",
      "activation_110 (Activation)      (None, 10)            0                                            \n",
      "====================================================================================================\n",
      "Total params: 115,114.0\n",
      "Trainable params: 113,546.0\n",
      "Non-trainable params: 1,568.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:142: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ac..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We implement here the basic Fabric design prescribed \n",
    "in the paper: CNF-dense with constant channels per scale.\n",
    "\n",
    "The only hyperparameters to this architecture are: number of \n",
    "layers, channels, scales, and the size of the convolution kernel.\n",
    "The size of the kernel is constant throughout the Fabric.\n",
    "The number of scales can, in fact, be autoset to reduce images \n",
    "to the coarsest resolution (1x1) on the last scale.\n",
    "\n",
    "Scale indices [0,1,..., num_scales-1] go from the \n",
    "highest resolution to the lowest resolution.\n",
    "An edge from a lower-index scale to a higher-index scale \n",
    "requires downsampling. We avoid max/average-pooling layers \n",
    "to downsample, preferring ReLU units and strided convolutions \n",
    "instead, as per the generalization results in the paper.\n",
    "Likewise, an edge from a higher-index scale to a \n",
    "lower-index requires upsampling: we repeat activations\n",
    "from the coarser level using the standard Keras implementation.\n",
    "\n",
    "After sizing, we apply batch normalization and a ReLU activation \n",
    "on each edge in the trellis.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'Rishabh Kabra'\n",
    "__email__ = 'rishabhkabra@gmail.com'\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, Activation, Flatten, Merge, Dense\n",
    "from keras.layers.merge import Add\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "num_channels = 16\n",
    "num_scales = 5\n",
    "num_layers = 4\n",
    "\n",
    "input_dim = 32,32,3\n",
    "inp = Input(input_dim)\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "def Upsample(input_shape, out_channels):\n",
    "    \"\"\"\n",
    "    Initialize a Fabric edge from a low-resolution scale\n",
    "    to the adjacent higher resolution, in the same layer \n",
    "    or between adjacent layers\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(UpSampling2D(size=(2, 2), input_shape=input_shape))\n",
    "    model.add(Conv2D(out_channels, kernel_size, strides=(1, 1), padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.1))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "    \n",
    "def Downsample(input_shape, out_channels):\n",
    "    \"\"\"\n",
    "    Initialize a Fabric edge from a high-resolution scale\n",
    "    to the adjacent lower resolution, in the same layer \n",
    "    or between adjacent layers\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(out_channels, kernel_size, strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization(momentum=0.1))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "    \n",
    "def SameRes(input_shape, out_channels):\n",
    "    \"\"\"\n",
    "    Initialize a Fabric edge between scales of the same\n",
    "    resolution, between adjacent layers\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(out_channels, kernel_size, strides=(1, 1), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization(momentum=0.1))\n",
    "    model.add(Activation('relu'))\n",
    "    return model\n",
    "\n",
    "fabric = {}\n",
    "# Each fabric[layer][scale] value is a list of Downsample/Upsample/SameRes modules, \n",
    "# representing all edges that may emerge from the Fabric node at [layer, scale].\n",
    "\n",
    "# Init: initialize all required edges at each node\n",
    "for layer in range(num_layers):\n",
    "    fabric[layer] = {}\n",
    "    for scale in range(num_scales):\n",
    "        input_shape = (input_dim[0]/int(2**scale), input_dim[1]/int(2**scale), num_channels)\n",
    "        if layer == 0:\n",
    "            fabric[layer][scale] = [Downsample(input_shape, num_channels)]\n",
    "        else:\n",
    "            input_shape = (32/int(2**scale), 32/int(2**scale), num_channels)\n",
    "            fabric[layer][scale] = [SameRes(input_shape, num_channels)]\n",
    "            if scale == 0:\n",
    "                fabric[layer][scale].append(Upsample(input_shape, num_channels))\n",
    "            elif scale < num_scales - 1:\n",
    "                fabric[layer][scale].append(Downsample(input_shape, num_channels))\n",
    "                fabric[layer][scale].append(Upsample(input_shape, num_channels))\n",
    "                if layer == num_layers - 1:\n",
    "                    fabric[layer][scale].append(Downsample(input_shape, num_channels))\n",
    "            else:\n",
    "                fabric[layer][scale].append(Downsample(input_shape, num_channels))\n",
    "                if layer == num_layers - 1:\n",
    "                    fabric[layer][scale].append(Downsample(input_shape, num_channels))\n",
    "                \n",
    "# Activations\n",
    "input_edge = SameRes(input_dim, num_channels) # edge from input into the Fabric\n",
    "init = input_edge(inp) # to be used as initial activation\n",
    "activation = {}\n",
    "for layer in range(num_layers):\n",
    "    activation[layer] = {}\n",
    "    for scale in range(num_scales):\n",
    "        if layer == 0:\n",
    "            if scale == 0:\n",
    "                activation[layer][scale] = fabric[layer][scale][0](init)\n",
    "            else:\n",
    "                activation[layer][scale] = fabric[layer][scale][0](activation[layer][scale-1])\n",
    "        else:\n",
    "            if scale == 0:\n",
    "                to_merge = []\n",
    "                to_merge.append(activation[layer-1][scale])\n",
    "                to_merge.append(fabric[layer][scale][1](activation[layer-1][scale+1]))\n",
    "            elif scale < num_scales-1:\n",
    "                to_merge = []\n",
    "                to_merge.append(activation[layer-1][scale])\n",
    "                to_merge.append(fabric[layer][scale][2](activation[layer-1][scale+1]))\n",
    "                to_merge.append(fabric[layer][scale][1](activation[layer-1][scale-1]))\n",
    "                if layer == num_layers-1:\n",
    "                    to_merge.append(fabric[layer][scale][3](activation[layer][scale-1]))\n",
    "            else:\n",
    "                to_merge = []\n",
    "                to_merge.append(activation[layer-1][scale])\n",
    "                to_merge.append(fabric[layer][scale][1](activation[layer-1][scale-1]))\n",
    "                if layer == num_layers-1:\n",
    "                    to_merge.append(fabric[layer][scale][2](activation[layer][scale-1]))\n",
    "            add_merged_inp = Add()(to_merge)\n",
    "            activation[layer][scale] = fabric[layer][scale][0](add_merged_inp)\n",
    "\n",
    "# Final output from the Fabric\n",
    "out = activation[num_layers-1][num_scales-1]\n",
    "out = Flatten()(out)\n",
    "out = Activation('softmax')(Dense(10)(out))\n",
    "\n",
    "model = Model(input=inp, output=out)  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final number of trainable parameters in our CNF-dense model, with 5 scales, 4 layers, and 16 channels, is 88k. As per Table 12 in the original paper, the CNF authors report 166k params in their model. We have half as many parameters as them. In the original paper, the number of scales is fixed at 1+log_2(dim_of_image) = 1+log_2(32) = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We fit the Fabric model using the Adam optimizer on all 50000 CIFAR10 training samples. We use a cross-entropy loss and train for accuracy. The images are pre-processed as in the paper: they can be randomly flipped horizontally or translated in either direction. We trained for 1000 epochs previously, but on this rerun we restrict ourselves to 100 epochs in the interest of time. The fitted model is saved as 'fabric_cifar10.h5' in the parent directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/risha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=2, validation_data=(array([[[..., steps_per_epoch=1000, epochs=100)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "179s - loss: 2.0613 - acc: 0.2478 - val_loss: 1.8119 - val_acc: 0.3490\n",
      "Epoch 2/100\n",
      "165s - loss: 1.7353 - acc: 0.3672 - val_loss: 1.5938 - val_acc: 0.4236\n",
      "Epoch 3/100\n",
      "166s - loss: 1.5939 - acc: 0.4153 - val_loss: 1.4873 - val_acc: 0.4577\n",
      "Epoch 4/100\n",
      "165s - loss: 1.5061 - acc: 0.4517 - val_loss: 1.4220 - val_acc: 0.4772\n",
      "Epoch 5/100\n",
      "166s - loss: 1.4390 - acc: 0.4737 - val_loss: 1.3473 - val_acc: 0.5064\n",
      "Epoch 6/100\n",
      "165s - loss: 1.3860 - acc: 0.4970 - val_loss: 1.2928 - val_acc: 0.5325\n",
      "Epoch 7/100\n",
      "165s - loss: 1.3395 - acc: 0.5147 - val_loss: 1.2666 - val_acc: 0.5377\n",
      "Epoch 8/100\n",
      "164s - loss: 1.3014 - acc: 0.5278 - val_loss: 1.2388 - val_acc: 0.5539\n",
      "Epoch 9/100\n",
      "164s - loss: 1.2649 - acc: 0.5420 - val_loss: 1.2149 - val_acc: 0.5629\n",
      "Epoch 10/100\n",
      "164s - loss: 1.2365 - acc: 0.5531 - val_loss: 1.2123 - val_acc: 0.5616\n",
      "Epoch 11/100\n",
      "164s - loss: 1.2082 - acc: 0.5650 - val_loss: 1.1395 - val_acc: 0.5840\n",
      "Epoch 12/100\n",
      "164s - loss: 1.1770 - acc: 0.5759 - val_loss: 1.1335 - val_acc: 0.5944\n",
      "Epoch 13/100\n",
      "164s - loss: 1.1590 - acc: 0.5838 - val_loss: 1.1031 - val_acc: 0.6050\n",
      "Epoch 14/100\n",
      "164s - loss: 1.1291 - acc: 0.5943 - val_loss: 1.0836 - val_acc: 0.6111\n",
      "Epoch 15/100\n",
      "164s - loss: 1.1028 - acc: 0.6050 - val_loss: 1.0574 - val_acc: 0.6197\n",
      "Epoch 16/100\n",
      "164s - loss: 1.0872 - acc: 0.6091 - val_loss: 1.0806 - val_acc: 0.6183\n",
      "Epoch 17/100\n",
      "164s - loss: 1.0712 - acc: 0.6193 - val_loss: 1.0368 - val_acc: 0.6324\n",
      "Epoch 18/100\n",
      "164s - loss: 1.0523 - acc: 0.6247 - val_loss: 1.0139 - val_acc: 0.6379\n",
      "Epoch 19/100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module is nearly identical to the CIFAR10 example \n",
    "in the keras package: see\n",
    "https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "\"\"\"\n",
    "\n",
    "__author__ = 'Rishabh Kabra'\n",
    "__email__ = 'rishabhkabra@gmail.com'\n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print 'X_train shape:', X_train.shape\n",
    "print X_train.shape[0], 'train samples'\n",
    "print X_test.shape[0], 'test samples'\n",
    "\n",
    "batch_size = 50\n",
    "nb_classes = 10\n",
    "nb_epoch = 100\n",
    "data_augmentation = True\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "sgd = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print 'Not using data augmentation.'\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "else:\n",
    "    print 'Using real-time data augmentation.'\n",
    "\n",
    "    # preprocessing and realtime data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # fit the model on the batches generated by datagen.flow()\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        samples_per_epoch=X_train.shape[0],\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=2)\n",
    "model.save('fabric_cifar10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot training history below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc', 'loss', 'val_acc', 'val_loss']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAF3CAYAAABnvQURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3cab61d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(hist.history['acc'], label='Training accuracy')\n",
    "ax.plot(hist.history['val_acc'], label='Test accuracy')\n",
    "ax.set_xlabel('Training epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model continues to gain accuracy on the test data through 100 epochs. When we ran 1000 epochs previously (no early stopping), we recorded a test accuracy maxima of about .80, which was very close to the 19.22% error rate recorded in the paper for 4 layers and 16 channels (Table 10).\n",
    "\n",
    "We recheck the accuracy achieved on the test set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7586\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import numpy as np\n",
    "pred = model.predict(X_test)\n",
    "classes_pred = np.argmax(expit(pred), axis=1)\n",
    "classes = np.argmax(Y_test, axis=1)\n",
    "print \"Accuracy:\", 1.*np.sum(classes_pred == classes)/classes.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pending work:\n",
    "\n",
    "Visualize weight of paths through the fabric.\n",
    "\n",
    "Try a higher res dataset, maybe ImageNet?\n",
    "\n",
    "Fit a deconvolutional network. Can it be trained adversarially?\n",
    "\n",
    "Test against other meta-learners that can optimize over a space of convolutional architectures:\n",
    "- Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing Neural Network Architectures using Reinforcement Learning. In ICLR '17: https://openreview.net/forum?id=S1c2cvqee. Use Q-learning with epsilon-greedy exploration strategy and experience replay. Trained on image classification tasks with 10 GPUs.\n",
    "- Barret Zoph and Quoc Le. Neural Architecture Search with Reinforcement Learning. In ICLR '17: https://openreview.net/forum?id=r1Ue8Hcxg. Higher scores and more thorough results than previous paper, but infrastructural requirements may make this infeasible to reproduce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
